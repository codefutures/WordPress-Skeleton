CREATE TABLE `wpag_rg_lead_detail_long` (  `lead_detail_id` bigint(20) unsigned NOT NULL,  `value` longtext,  PRIMARY KEY (`lead_detail_id`)) ENGINE=MyISAM DEFAULT CHARSET=utf8;
/*!40000 ALTER TABLE `wpag_rg_lead_detail_long` DISABLE KEYS */;
INSERT INTO `wpag_rg_lead_detail_long` VALUES('42', 'I saw the talk on AgilData at the Cloud Compute Expo. My primary question is how exactly does the data structure help me over something like say PostgresSQL. I primarily work with relational data and this feels a lot like a nosql database which I have trouble seeing as a good replacement for a relational database.\r\n\r\nIs there any reading you have regarding the vision you had for how the database is intended to be used? There were some concepts mentioned like not needing data migrations and a \"flexible schema\" which sounds great depending on the details which I would love to look into.');
INSERT INTO `wpag_rg_lead_detail_long` VALUES('61', 'I am an IT analyst for Ovum who first defined the market segment called Fast Data 3 years ago. http://www.zdnet.com/article/fast-data-hits-the-big-data-fast-lane/ I am compiling an update on streaming  and would like to speak with your new management team.');
INSERT INTO `wpag_rg_lead_detail_long` VALUES('91', 'Greetings to this lovely company.  kings solutions is a part of the  eCommerce industry, we offer solutions to software needs and in order to increase our customer base and success, we decided to seek for data from reputable company like yours. we are specifically in need of email data for email marketing purpose, or a link we could get these data mostly email addresses in millions of numbers are most likely needed by us. This is the reason we contacted this firm, and we are desperately waiting for a positive reply.');
INSERT INTO `wpag_rg_lead_detail_long` VALUES('171', 'I am looking for work part time full time whatever.  My resume is below:\r\nERIC SOUARE\r\n\r\nBIG DATA CONSULTANT\r\n\r\neric@bicyclebi.com\r\n\r\n\r\n\r\nSkills\r\n\r\n\r\n● 	Big Data : Spark (SQL,Streaming,ML,Core), Hadoop, MapReduce, Cassandra, Hbase, Kafka, Storm, RabbitMQ, Hive, Pig, Impala, PrestoDB, BigQuery, Elasticsearch, ELK, MongoDB\r\n●    Cloud : Amazon Web Services, Google Cloud Environment, RackSpace, Windows Azure\r\n●    Languages : Java, Scala, Python basic’s.\r\n●    Cloudera, Hortonworks, MapR, Custom Apache distributions\r\n●    Others : Spring Framework, RESTful Web services, Basics Front­End knowledge\r\n● 	Data : E­Commerce, Social Media, Logs and click events data, Next Generation Genomic Data, Oil & Gas, Healthcare, Travel.\r\n\r\nSummary: Experienced Big Data consultant with 9+ years’ hands on, end to end enterprise applications implementation using Big Data technologies.  Familiar with Big Data infrastructure, Data ingestion techniques and Data Analytics models. Experience with Technical Program Management and knowledge of product lifecycle development. Good understanding of Hadoop deployment architecture definitions and documentation for Hadoop based production environment scalable to petabytes. Competent to deploy, administer and manage Hadoop Software on large cluster implementations.  \r\n\r\nEducation\r\n\r\n\r\nMaster IT Project Management \r\nDepaul University\r\n2014\r\n\r\nBachelor of Science - Management\r\nEmbry Riddle Aeronautical University\r\n1996\r\n\r\nBig Data Certifications:\r\nDataBricks/O\'Reilly  Apache Spark Developer Certification – 04­2015\r\n\r\nBank of America – Big Data Consultant - Sept 2009 to present\r\n\r\n● 	Worked on a Big Data analytics solutions that can be easily customized to the customer’s specific business needs and seamlessly integrated into their existing infrastructure. ­­ Online retail analytics ­­ Digital advertisements analytics ­­ Traffic Data analytics.\r\n● 	Deploy  Big Data technologies like Apache Spark, Hadoop, MapReduce, Storm, Kafka, HDFS, Hbase,Hive, Shark, BigQuery, Spring Data, Cassandra, Social Media Data, Amazon Web Services, Rackspace, Google Cloud.\r\n●    Successful implemented Big Data applications on different cloud environments.\r\n●	Project E­Commerce Search Engine : Used Big Data technologies like Cassandra, Hive, Spark SQL , Spark Machine Learning Library, Hadoop, MapReduce, Amazon Web Services, CDN, Scala, Java, Developed a Spark Streaming_Kafka App to Process Hadoop Jobs Logs, Implemented Kafka Producer to send all slaves logs to Spark Streaming app, Written Spark Streaming app to process the logs with given rules and produce the bad images, bad records, missed records etc, Developed Spark Streaming App collect user actions data from front end, Kafka Producer based Rest API to collect user events and send to Spark Streaming App, Hive Queries to Generate Stock \r\nAlerts, Price Alerts, Popular Products Alerts, New Arrivals for each user based on given likes, favorite shares counts information, Working on Spark MLlib for Recommendations, Coupons Recommendations, Rules Engine\r\n●    Project Truck Events Analysis : Big Data technologies used Hadoop, HDFS, Hive, HBase, Kafka,\r\nStorm, RabbitMQ Web Stomp, Google Maps and Written a simulator to send/emit events based on NYC DOT data file, Written Kafka Producer to accept/send events to Kafka Producer which is on Storm Spout, Written Storm topology to accept events from Kafka Producer and Process Events, Written Storm Bolt to Emit data into Hbase, HDFS, RabbitMQ Web Stomp, Hive Queries to Map Truck Events Data , Weather Data, Traffic Data.\r\n●    Project Comparative Analysis of Big Data Analytical Tools : Using Hive,Hive on Tez, Impala, Spark\r\nSQL, Apache Drill, BigQuery, PrestoDB running on the Google Cloud and AWS.\r\n●	Project Cimbal/MobApp Pay : Cimbal is a mobile promotion and payment network designed to increase business sales and deliver targeted deals to consumers. Used Hadoop MapReduce, Hbase , Spring Data Rest Web Service, CDH and I have Written MapReduce programs to validate the data, Written more than 50 Spring Data Hbase rest API\'s in Java, Schema design on Hbase and cleaning data, and also Written Hive queries for analytics on users data.\r\n\r\n\r\nPolarys - Big data Consultant, Feb 2008 – Sept 2009\r\n\r\nPolarys datawarehouse was processing claims data from several regions in France, having source of the data from PegaSystems and target into IBM DB2 pure XML. But, the processing time with this system was long. A new EDW was build using Hadoop ecosystem which reduced the processing time of the data. In the new system data was loaded into Hive instead of IBM DB2 pure XML.  \r\nUsing Hadoop ecosystem Data is first landed into Hadoop Data Lake from PegaSystem source where it is transformed using MapReduce and Pig and finally loaded into Hive external tables. On top of Hive, Cognos reporting is done to analyze the data.  \r\nWith the new EDW data processing time was reduced to 3hr as compare to earlier 24hrs. \r\n \r\nResponsibilities: \r\n• Writing Map reduce code. \r\n• Writing PIG scripts. \r\n• Writing Hive queries. \r\n• Writing MR Unit test cases for map reduce job. \r\n• Writing PIG Unit test cases for PIG scripts. \r\n• Writing shell scripting\r\n\r\n\r\n\r\nPositive Bioscience – Big Data/ Hadoop Developer - April 2006 – Jan 2008                         \r\n\r\n●	Design and development of software for Bioinformatics, Next Generation Sequencing (NGS) in Hadoop MapReduce framework, MongoDB using Amazon S3, Amazon EC2, Amazon Elastic MapReduce(EMR).\r\n●   Developed MapReduce programs to perform Quality Check, Sequence Alignment, SNP calling,\r\nSV/CNV detection on single-end/paired-end NGS data.\r\n●   Designed and transmitted a RDBMS (SQL) Database to NOSQL MongoDB Database.\r\n●   Developed Hadoop MapReduce program to perform custom Quality Check on genomic data.\r\nNovel features of the program included capability to handle file-format/sequencing-machine errors, automatic detection of baseline PHRED score and being platform agnostic (Illumina,\r\n454 Roche, Complete Genomics, ABI Solid input format data).\r\n●	Developed a Hadoop MapReduce program to perform sequence alignment on sequencing data. The MapReduce program implements algorithms such as Burrows-Wheeler Transform (BWT), Ferragina-Manzini Index (FMI), Smith-Waterman dynamic programming algorithm using Hadoop distributed cache.\r\n●	Configured and ran all MapReduce programs on 20-30 node cluster (Amazon EC2 spot instances) with Apache Hadoop-1.4.0 to handle 600GB/sample of NGS genomics data.\r\n●   Configured a 20-30 node (Amazon EC2 spot Instance) Hadoop cluster to transfer the data from\r\nAmazon S3 to HDFS and HDFS to Amazon S3 and also to direct input and output to the\r\nHadoop MapReduce framework.\r\n●		successfully ran all Hadoop MapReduce programs on Amazon Elastic MapReduce framework by using Amazon S3 for Input and Output.\r\n●   Developed java RESTful web services to upload data from local to Amazon S3, listing S3\r\nObjects and file manipulation operations.\r\n\r\n\r\n\r\nPointCross.com – SQL Developer  -  Jan 2002 – March 2006                           \r\n\r\n\r\n•	Created SQL scripts and written Procedures using PL/SQL to load data from flat files into new tables (using both UTL and SQL Loader - for different cases) \r\n•	 Explain Plan, TKPROF  used for checking the performance of queries . Modified and updated existing custom forms to adapt to new database schema \r\n•	Involved in creation and updates of several SQL packages and Procedures \r\n•	Created SQL scripts for conversion of legacy data (including validations) and then load it into the tables \r\n•	Developed SQL*Loader programs &PL/SQL scripts to load legacy data into Oracle Apps. \r\n•	Developed reports using Reports 6i. \r\n•	Developed PL/SQL programs for various modules to validate data while importing into oracle system. \r\n•	Participated in database development like creating PL/SQL triggers, packages, procedures function. \r\n•	Handling errors using Exceptions. \r\n\r\n\r\nUS CITIZEN');
INSERT INTO `wpag_rg_lead_detail_long` VALUES('176', 'Hey,\r\n\r\nI saw that you were looking for various developers. I\'m an Associate consultant at VenturePact - a New York-based, invite-only marketplace that connects companies with pre-screened developers and designers based on their specific needs across the globe. I would be happy to help you find the developers which fit your budget and location to help you execute your requirements. Plus, developers pay us to be part of our network - so we won\'t charge you anything. \r\n\r\n\r\nPlease let me know if you want to hop on a quick call. Really looking forward to hearing from you. :)');
INSERT INTO `wpag_rg_lead_detail_long` VALUES('233', 'Hi,\r\n\r\nWe provide ready to use email lists of prospects. Would you be interested in acquiring the same for your lead generation purpose? We can provide contacts based on job titles, industries, technology used, revenue and employee size, geography and more.\r\n\r\nPlease let me know your target audience and few free samples would be sent for your review.\r\n\r\nYour quick note will be greatly appreciated.\r\n\r\nRegards,\r\nKristina');
INSERT INTO `wpag_rg_lead_detail_long` VALUES('354', 'Hi,\r\n\r\nWould you be interested in acquiring the most recently updates list of Cloud Computing User Accounts - Q4 2015?\r\n\r\nData Fields Provided: Company Name, Web Address, Contact Name, Verified Email, Job Title, Application Type, Complete Mailing Address, Phone Number, FAX Number, Total Employees, SIC Code, and Industry details.\r\n\r\nSimilar User Accounts that might interest you: \r\n\r\n•	Infor\r\n•	SAP\r\n•	ERP\r\n•	Epicor\r\n•	Oracle\r\n•	Microsoft\r\nPlease do let me know if this interests you to take the discussion forward.  Looking forward to your response.');
INSERT INTO `wpag_rg_lead_detail_long` VALUES('612', 'Hi Daniel, \r\nWould you consider speaking at the Denver chapter of the IIBA? I\'m the VP of Education and am responsible for finding speakers. I also attended Mile High Agile and I know you spoke there on Breaking Down Technically Complex Stories. I think the topic would be relevant as well for the Business Analyst group. We meet every other month, on the third Wednesday of the month, and we are currently looking for speakers for January 20 and March 16. Please feel free to call or e-mail or text. My mobile # is 720-382-9765. Thanks in advance for considering!! Have a great rest of the week.  \r\nHeidi');
/*!40000 ALTER TABLE `wpag_rg_lead_detail_long` ENABLE KEYS */;
